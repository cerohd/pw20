<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Patrick Weaver: Blog</title>
  <subtitle></subtitle>
  <link href="https://patrickweaver.net/feed.xml" rel="self"/>
  <link href="https://patrickweaver.net/blog/"/>
  <updated>2020-10-21T00:00:00-00:00</updated>
  <id>https://patrickweaver.net/</id>
  <author>
    <name>Patrick Weaver</name>
    <email>hello.patrickw@gmail.com</email>
  </author>
  
  <entry>
    <title>How we used a chatbot to teach writing and coding!</title>
    <link href="https://patrickweaver.net/blog/how-we-used-a-chatbot-to-teach-writing-and-coding/"/>
    <updated>2016-06-15T00:00:00-00:00</updated>
    <id>https://patrickweaver.net/blog/how-we-used-a-chatbot-to-teach-writing-and-coding/</id>
    <content type="html">&lt;p&gt;When &lt;a href=&quot;https://mouse.org/&quot;&gt;Mouse&lt;/a&gt; moved into our new office in early 2015, we were excited to have space for our high school &lt;a href=&quot;https://mouse.org/mouse-design-league&quot;&gt;Design League&lt;/a&gt; program to spread out and work on assistive technology projects like &lt;a href=&quot;https://www.instagram.com/p/BGiUJ8awRJp/&quot;&gt;1derphone&lt;/a&gt;, a motorized headphone for a DJ with limited mobility. We also quickly realized we had the chance to reach more students in more ways by hosting flexible open events that any NYC students could attend.&lt;/p&gt;
&lt;p&gt;We hosted our first open &lt;a href=&quot;https://mouse.org/makernight&quot;&gt;Maker Night&lt;/a&gt; in March 2015 and it quickly became a space where students showed up and could work on anything creative and innovative. Every month, we saw new projects emerge from crafting, coding, and anything in between:&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*eku7Cc10Y9NSFAm7MP7tYA.jpeg&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/400/1*hA1_vnNXOvLg0WCWt6PluA.jpeg&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*ieyZhs_PDEsjWpTP8sDWaw.jpeg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;figcaption&gt;
&lt;p&gt;Pictures from Maker Night at &lt;a href=&quot;http://mouse.org/&quot;&gt;Mouse&lt;/a&gt;&lt;/p&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We often test ideas for new creative activities at Maker Nights and one new project I had been playing around with was &lt;a href=&quot;https://forest-noodle.glitch.me/&quot;&gt;a simple chatbot&lt;/a&gt; that students could remix. We love teaching any new tech skill through activities that are quick and easy to jump into and have created a few introduction to &lt;a href=&quot;https://zircon-porch.glitch.me/&quot;&gt;coding&lt;/a&gt; &lt;a href=&quot;https://distinct-zoo.glitch.me/&quot;&gt;activities&lt;/a&gt; using remixable JavaScript arrays (Check out a few of &lt;a href=&quot;https://automatic-triangle.glitch.me/&quot;&gt;Mouse‚Äôs Web Making activities&lt;/a&gt; created through our partnership with Mozilla).&lt;/p&gt;
&lt;p&gt;We usually have students start out by quickly &lt;a href=&quot;https://efficacious-hook.glitch.me/&quot;&gt;remixing the chatbot‚Äôs script&lt;/a&gt;: have the bot ask about your day, or give the chatbot a unique personality. Then they can move on to second level challenges: &lt;a href=&quot;https://smart-raincoat.glitch.me/&quot;&gt;remix the interface CSS&lt;/a&gt;, &lt;a href=&quot;https://possible-boot.glitch.me/&quot;&gt;teach the bot to repeat a user‚Äôs name back to them&lt;/a&gt;, or &lt;a href=&quot;https://obtainable-mosquito.glitch.me/&quot;&gt;teach the bot to respond to yes or no questions&lt;/a&gt;. All the linked examples are real student work, created between pizza slices at a Mouse Maker Night!&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*R7wDi-aaq56c6scszMkT9g.png&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*4frB9lA0wqCuoQMb_lLdHQ.png&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*B2IS5kkuK6FaQjmIsPJ3CA.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;figcaption&gt;3 remixes of the chatbot&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;I tried out the chatbot at our March Maker Night with a few students from &lt;a href=&quot;http://lespnyc.com/&quot;&gt;Lower East Side Prep&lt;/a&gt;. Many LESP students are recent immigrants and as their principal notes on their website, their ‚Äústudent body represents more than 50 countries including the United States.‚Äù I sat down with a few LESP students, showed them the basics of editing the script and they instantly started creating.&lt;/p&gt;
&lt;p&gt;Students who were shy to get started or were nervous that they had never coded before were able to get familiar by creating linear scripts that anticipated a user‚Äôs responses or building in simple logic. One student who had recently moved to the U.S. created a bot that &lt;a href=&quot;https://lucky-bassoon.glitch.me/&quot;&gt;empathized with people who miss faraway friends&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Something about the narrative and engaging interface of a bot made students want to iterate on their designs. Two students would write a bot script, then switch and play each other‚Äôs bots. They would start by focusing on writing a friendly personality, then dive into a challenge like the bot learning a user‚Äôs name or a place name.&lt;/p&gt;
&lt;p&gt;Moments like these are exactly what Mouse strives to create with technology focused youth development curriculum. We love seeing language arts students publish writing online and code their own websites, history students create a &lt;a href=&quot;https://www.youtube.com/watch?v=5D1iIEb49lE&quot;&gt;Serious Game&lt;/a&gt; about the civil rights movement, or science students make DIY Batteries to learn &lt;a href=&quot;https://www.youtube.com/watch?v=nofdW8nARTg&quot;&gt;Green Tech&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;üì† Interested in learning more about Mouse? Email &lt;a href=&quot;mailto:info@mouse.org&quot;&gt;info@mouse.org&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;üèÖ Check out the &lt;a href=&quot;http://makernighthalloffame.tumblr.com/&quot;&gt;Maker Night Hall of Fame on Tumblr&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;üóΩ In the NYC area and interested in attending a Mouse Maker Night?&lt;/strong&gt; &lt;a href=&quot;http://mouse.org/makernight&quot;&gt;Sign up&lt;/a&gt; for our next Maker Night on &lt;strong&gt;June 30 from 4pm to 6pm&lt;/strong&gt;!&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>Which Wearable Arduino Should I Use For My Sewable Tech Project?</title>
    <link href="https://patrickweaver.net/blog/which-wearable-arduino-should-i-use-for-my-sewable-tech-project/"/>
    <updated>2017-09-29T00:00:00-00:00</updated>
    <id>https://patrickweaver.net/blog/which-wearable-arduino-should-i-use-for-my-sewable-tech-project/</id>
    <content type="html">&lt;p&gt;This fall &lt;a href=&quot;https://medium.com/@mouse_org&quot;&gt;Mouse&lt;/a&gt; (&lt;a href=&quot;http://mouse.org/&quot;&gt;mouse.org&lt;/a&gt;) launched our brand new &lt;a href=&quot;https://mouse.org/mouse-courses&quot;&gt;Sewable Tech Course&lt;/a&gt;! The course introduces learners to circuitry and electronics without the abstraction of a breadboard. In the final two projects in the course we use a wearable Arduino to prototype a DIY activity tracker.&lt;/p&gt;
 &lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*gfNKPipgo0Yp_bZLJLg7MQ.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;figcaption&gt;Sew-matic for DIY Wearable Goal Tracker from Mouse Create&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The big question we ran into when I was helping my colleague &lt;a href=&quot;https://twitter.com/katermouse&quot;&gt;Kate&lt;/a&gt; prototype the course was, which wearable Arduino to use. Along the way we tested 7 Arduino compatible microcontrollers and settled on recommending a &lt;a href=&quot;https://www.sparkfun.com/products/12049&quot;&gt;LilyPad Arduino USB&lt;/a&gt;, but there are advantages to each of the Arduinos we tested and different kinds projects would benefit from including each of them.&lt;/p&gt;
 &lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*nxNOfE_aAIHrgXcgX8GSLQ.jpeg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;figcaption&gt;Our collection of wearable arduinos&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://www.sparkfun.com/products/12049&quot;&gt;Sparkfun LilyPad Arduino USB&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.ebay.com/sch/i.html?_nkw=LilyPad+Arduino+USB+ATmega32U4&quot;&gt;Generic LilyPad Arduino USB&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.adafruit.com/product/659&quot;&gt;Adafruit FLORA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.ebay.com/sch/i.html?_nkw=LilyTiny&quot;&gt;Generic LilyTiny&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.adafruit.com/product/2470&quot;&gt;Arduino GEMMA&lt;/a&gt; (Discontinued)&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.adafruit.com/product/1222&quot;&gt;Adafruit GEMMA v2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.adafruit.com/product/3501&quot;&gt;Adafruit GEMMA M0&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;All of the boards work with the &lt;a href=&quot;https://www.arduino.cc/en/Main/Software&quot;&gt;Arduino IDE&lt;/a&gt; (most with some extra steps), are roughly circular, and most run on 3.3 Volts, but there are slight differences that make certain boards better for some types of projects.&lt;/p&gt;
&lt;h4&gt;[1] &amp;amp; [2] Sparkfun/Generic LilyPad Arduino USB&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*FWirsLamzgR-H2Wbht2ZYg.jpeg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[1] From &lt;a href=&quot;https://www.sparkfun.com/products/12049&quot;&gt;Sparkfun: $25&lt;/a&gt;, [2] &lt;a href=&quot;https://www.ebay.com/sch/i.html?_nkw=LilyPad+Arduino+USB+ATmega32U4&quot;&gt;Generic: $6 to $20&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;5 Digital Pins&lt;/li&gt;
&lt;li&gt;4 Analog Pins&lt;/li&gt;
&lt;li&gt;Onboard LED (D13)&lt;/li&gt;
&lt;li&gt;On/Charge Switch and Reset Button&lt;/li&gt;
&lt;li&gt;JST battery connector&lt;/li&gt;
&lt;li&gt;2&amp;quot;/5 cm&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.arduino.cc/en/Reference/Serial&quot;&gt;Supports Serial Communication&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The LilyPad Arduinos are the only boards on this list that work with the Arduino IDE without any extra steps, this is the main reason why we recommend this board on Mouse Create. If this board will be your first experience with Arduino or you will need a few analog pins the LilyPad would be a great choice.&lt;/p&gt;
&lt;p&gt;The SparkFun model costs a bit more but a portion of the sales go to &lt;a href=&quot;http://leahbuechley.com/&quot;&gt;Leah Buechley&lt;/a&gt; to support open source e-textiles and e-textiles education development.&lt;/p&gt;
&lt;p&gt;Be careful when buying to get the LilyPad Arduino USB if you need a USB connection. The original &lt;a href=&quot;https://www.sparkfun.com/products/13342&quot;&gt;LilyPad Arduino 328 Main Board&lt;/a&gt; (not pictured), requires an FTDI connection to upload code (but that one has more pins!).&lt;/p&gt;
&lt;h4&gt;[3] Adafruit FLORA&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*WOH8dSMPKiHUzYIAU4WerQ.jpeg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;From &lt;a href=&quot;https://www.adafruit.com/product/659&quot;&gt;Adafruit $15&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;8 Digital Pins&lt;/li&gt;
&lt;li&gt;4 Analog Pins (D6, D9, D10, D12)&lt;/li&gt;
&lt;li&gt;Onboard LED (D7), Onboard RGB LED (D8)&lt;/li&gt;
&lt;li&gt;On/Off Switch and Reset Button&lt;/li&gt;
&lt;li&gt;JST battery connector&lt;/li&gt;
&lt;li&gt;1.75&amp;quot;/4.5 cm&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.arduino.cc/en/Reference/Serial&quot;&gt;Supports Serial Communication&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Adafruit Flora is a great board and can handle any project. It‚Äôs smaller than the LilyPad but has 3 more pins, and the onboard RGB LED is very useful as a Hello, World. The only drawback to this board is &lt;a href=&quot;https://learn.adafruit.com/add-boards-arduino-v164/overview&quot;&gt;adding the board in the Arduino IDE&lt;/a&gt; and installing the &lt;a href=&quot;https://learn.adafruit.com/adafruit-arduino-ide-setup/windows-driver-installation&quot;&gt;Adafruit drivers on Windows&lt;/a&gt;, but both of these are one-time steps and don‚Äôt take more than a few minutes. We did have some issues uploading code via a USB 3.0 port, but this can be avoided by using a USB 2.0 hub.&lt;/p&gt;
&lt;h4&gt;[4] Generic LilyTiny&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*6Cx0oeFjZ0aGB6ek2vzCtQ.jpeg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.ebay.com/sch/i.html?_nkw=LilyTiny&quot;&gt;Generic $2 to $5&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;6 Digital Pins&lt;/li&gt;
&lt;li&gt;Onboard LED (D1)&lt;/li&gt;
&lt;li&gt;1&amp;quot;/2.5 cm&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We found this board when researching and thought it might be useful for large groups with a limited budget to give every learner a board. This board has two easy to recognize advantages, it is both the smallest and the least expensive board on this list. The disadvantages are a little bit harder to spot so if you are thinking of using this board with a large group I would buy one to test with first.&lt;/p&gt;
&lt;p&gt;This board is the only one on this list that does not have alligator clip friendly pins. When testing we were still able to attach alligator clips, but it is easy to accidentally clip two pins and create a short circuit. This board also lacks both an on/off switch and a JST battery connector which means it has to be powered via USB or the 5V pin (this is the only board on this list that runs on 5V).&lt;/p&gt;
&lt;p&gt;The biggest disadvantage of this board is the upload process. &lt;a href=&quot;https://digistump.com/wiki/digispark/tutorials/connecting&quot;&gt;Following these instructions&lt;/a&gt;, we had to add the board to the Arduino IDE, install Digistump drivers, and hit the plug in the board to USB at a specific point in the process. We also had issues with this board on USB 3.0 ports.&lt;/p&gt;
&lt;p&gt;If you have a lot of experience with Arduino or have a very small budget but a lot of patience this board might be for you.&lt;/p&gt;
&lt;h4&gt;[5] &amp;amp; [6] Arduino GEMMA and Adafruit GEMMA v2&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*MyLI2jE1Oo2yA6sYbYQKCA.jpeg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[6] From &lt;a href=&quot;https://www.adafruit.com/product/1222&quot;&gt;Adafruit $10&lt;/a&gt;, [5] From &lt;a href=&quot;https://www.adafruit.com/product/2470&quot;&gt;Adafruit (Discontinued)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3 Digital Pins&lt;/li&gt;
&lt;li&gt;1 Analog Pin (D2)&lt;/li&gt;
&lt;li&gt;Onboard LED (D1)&lt;/li&gt;
&lt;li&gt;On/Off Switch and Reset Button&lt;/li&gt;
&lt;li&gt;JST battery connector&lt;/li&gt;
&lt;li&gt;1.25&amp;quot;/2.75 cm&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both of these boards (along with the Gemma M0) are manufactured by Adafruit, and are visually and functionally very similar. The Arduino Gemma is teal and has the Adafruit logo on the back, the Adafruit Gemma v2 is black and has the Adafruit logo on the front. Both have an ATtiny85 chip that does not support serial communication.&lt;/p&gt;
&lt;p&gt;Both the Gemma boards are great small wearable Arduinos. The small number of pins and small size make this board very approachable for a beginner, and would make the perfect tool for a simple sewable tech project. We initially were going to recommend the Adafruit Gemma v2 on Mouse Create, but we had enough &lt;a href=&quot;https://learn.adafruit.com/introducing-gemma/about-the-bootloader&quot;&gt;issues with uploading via USB 3.0&lt;/a&gt; that instead of shipping a USB 2.0 hub along with the boards we decided to opt for the LilyPad.&lt;/p&gt;
&lt;p&gt;If you already have a USB 2.0 hub or computers with USB 2.0 ports this board will work great. It does not have serial communication so it is best suited for simpler projects. Like the Adafruit Flora you will have to &lt;a href=&quot;https://learn.adafruit.com/add-boards-arduino-v164/overview&quot;&gt;add the board to the Arduino IDE&lt;/a&gt; and &lt;a href=&quot;https://learn.adafruit.com/adafruit-arduino-ide-setup/windows-driver-installation&quot;&gt;install the Adafruit drivers on Windows&lt;/a&gt; (for the Gemma V2).&lt;/p&gt;
&lt;h4&gt;[7] Adafruit GEMMA M0&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*Qyf4eRy8XKT-_OwYjHu9xw.jpeg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;From &lt;a href=&quot;https://www.adafruit.com/product/3501&quot;&gt;Adafruit $10&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3 Digital Pins&lt;/li&gt;
&lt;li&gt;3 Analog Pins (D0, D1, D2)&lt;/li&gt;
&lt;li&gt;Onbaord LED (D13), Onboard RGB LED (D?)&lt;/li&gt;
&lt;li&gt;On/Off Switch and Reset Button&lt;/li&gt;
&lt;li&gt;JST battery connector&lt;/li&gt;
&lt;li&gt;1.25&amp;quot;/2.75 cm&lt;/li&gt;
&lt;li&gt;Supports &lt;a href=&quot;https://www.arduino.cc/en/Reference/Serial&quot;&gt;Serial Communication&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Supports &lt;a href=&quot;https://blog.adafruit.com/2017/01/09/welcome-to-the-adafruit-circuitpython-beta/&quot;&gt;Circuit Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Adafruit GEMMA M0 was released just as we were finishing up our Sewable Tech course, a little too late to include in the kits we were putting together, but I still highly recommend this board. The Gemma M0 does everything the older Gemmas can and a lot more. I haven‚Äôt had any issues with USB 3.0 using this board, and the ATSAMD21E18 chip supports serial communication.&lt;/p&gt;
&lt;p&gt;The big step forward with the Gemma M0 and the reason Mouse might include this board in future kits is &lt;a href=&quot;https://blog.adafruit.com/2017/01/09/welcome-to-the-adafruit-circuitpython-beta/&quot;&gt;Circuit Python&lt;/a&gt;. A frequent question from schools interested in circuitry and electronics and sewable tech is ‚ÄúWill this Arduino work with my Chromebook?‚Äù Until now the answer has always been no (though the &lt;a href=&quot;https://www.arduino.cc/en/Main/Software&quot;&gt;online Arduino IDE&lt;/a&gt; now works with Arduino Uno).&lt;/p&gt;
&lt;p&gt;Circuit Python allows you to use the Gemma M0 to bypass the Arduino IDE completely (but still works with the IDE if you prefer that). The board will show up as a drive when you plug it in to your computer (yes, even a Chromebook), and adding the following code to a file called &lt;code&gt;code.py&lt;/code&gt; will produce the familiar ‚ÄúBlink‚Äù Hello, World example.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import digitalio
import board
import time
led = digitalio.DigitalInOut(board.D13)
led.direction = digitalio.Direction.OUTPUT
while True:
    led.value = not led.value
    time.sleep(0.5)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As with other Adafruit Boards if you are using the Arduino IDE instead of Python you will have to &lt;a href=&quot;https://learn.adafruit.com/add-boards-arduino-v164/overview&quot;&gt;add the board to the IDE&lt;/a&gt; and &lt;a href=&quot;https://learn.adafruit.com/adafruit-arduino-ide-setup/windows-driver-installation&quot;&gt;install the Adafruit drivers on Windows&lt;/a&gt;. You may also have to &lt;a href=&quot;https://learn.adafruit.com/adafruit-gemma-m0/circuitpython&quot;&gt;reinstall Circuit Python&lt;/a&gt; if you go back and forth between the IDE and Circuit Python, but this is a simple drag and drop process. I‚Äôm also not sure how to control the RGB LED but I haven‚Äôt spent much time trying to figure it out.&lt;/p&gt;
&lt;p&gt;All the boards we tested worked great once we figured out the tricks of each and any will work for most sewable tech projects. Once you decide on a board, check out &lt;a href=&quot;https://mouse.org/work&quot;&gt;Mouse Create&lt;/a&gt; to find some fun projects!&lt;/p&gt;
 &lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*dNpQjlUdHIzaTusjw9ERRw.jpeg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;figcaption&gt;
&lt;p&gt;Making a light up phone case at &lt;a href=&quot;https://medium.com/@mouse_org&quot;&gt;Mouse&lt;/a&gt; &lt;a href=&quot;https://mouse.org/makernight&quot;&gt;Maker Night&lt;/a&gt;&lt;/p&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;</content>
  </entry>
  
  <entry>
    <title>Building a futuristic Record Player with Glitch and Raspberry Pi</title>
    <link href="https://patrickweaver.net/blog/building-a-futuristic-record-player-with-glitch-and-raspberry-pi/"/>
    <updated>2018-10-23T00:00:00-00:00</updated>
    <id>https://patrickweaver.net/blog/building-a-futuristic-record-player-with-glitch-and-raspberry-pi/</id>
    <content type="html">&lt;p&gt;Earlier this year I wanted to explore the new &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/async_function&quot;&gt;async/await&lt;/a&gt; functionality in Javascript so I started playing around with &lt;a href=&quot;https://async-await-machine.glitch.me/&quot;&gt;a project&lt;/a&gt; on Glitch (&lt;a href=&quot;https://glitch.com/&quot;&gt;glitch.com&lt;/a&gt;) that would call one API after another, then generate new API call options from each cycle. I compiled a long list of potential APIs to use, but didn‚Äôt get past chaining together an &lt;a href=&quot;https://dog.ceo/dog-api/&quot;&gt;API that will respond with a picture of a specific breed of dog&lt;/a&gt;, and the &lt;a href=&quot;https://www.mediawiki.org/wiki/API:Main_page&quot;&gt;Wikipedia API&lt;/a&gt; which could respond with the pages that came up in a search for the name of the breed.&lt;/p&gt;
&lt;figure&gt;
&lt;div class=&quot;glitch-embed-wrap&quot; style=&quot;height: 420px; width: 100%;&quot;&gt;
  &lt;iframe allow=&quot;geolocation; microphone; camera; midi; encrypted-media&quot; src=&quot;https://glitch.com/embed/#!/embed/async-await-machine?path=README.md&amp;previewSize=100&quot; alt=&quot;async-await-machine on Glitch&quot; style=&quot;height: 100%; width: 100%; border: 0;&quot;&gt;
  &lt;/iframe&gt;
&lt;/div&gt;
&lt;figcaption&gt;A Glitch embed of my Dog ‚Üí Wikipedia API prototype: click on the name of a dog and you will get a picture of that dog, and a list of pages linked from that dog‚Äôs Wikipedia page.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;I started exploring what other APIs I could connect and realized that most of them seemed to enforce a relatively strict internal type system. Even from the many connections Wikipedia could provide it was hard to think of a potential next link that would always match up with the mishmash Wikipedia returned. While it might be easy to match up the Google Maps API to a weather API, connecting more abstract concepts was much more difficult.&lt;/p&gt;
&lt;h2&gt;The ‚ÄòRecord Player‚Äô app&lt;/h2&gt;
&lt;p&gt;While thinking through this an idea jumped out at me. I could connect the Google Vision API to Spotify to find albums based on pictures of a record cover. The idea seemed so obvious that I figured someone else had already done it (with more thorough research later I found a few similar projects but none that were fully implemented).&lt;/p&gt;
&lt;p&gt;I decided that this spark of an idea was already a lot more interesting than my infinite API art project, but it still had the Rube Goldberg-esque quality I was going for. Even so, I still might have abandoned it without a tool like Glitch, which to me says, ‚Äúyour wacky idea is worth making (and it won‚Äôt take very long).‚Äù Using the skeleton of my previous project, I was able to put together (in about an hour) an app that bounced an image off of the Google Vision API then brought up search results from Spotify. I sent it to a friend who works at Spotify, and when she confirmed that it didn‚Äôt already exist I decided to spend a few hours putting together a more polished version (the hardest part turned out to be drag and drop file upload).&lt;/p&gt;
&lt;p&gt;Thanks to Glitch‚Äôs embed feature you can take a look at the app (and the code!) below:&lt;/p&gt;
&lt;figure&gt;
&lt;div class=&quot;glitch-embed-wrap&quot; style=&quot;height: 420px; width: 100%;&quot;&gt;
  &lt;iframe allow=&quot;geolocation; microphone; camera; midi; encrypted-media&quot; src=&quot;https://glitch.com/embed/#!/embed/record-player?path=README.md&amp;previewSize=100&quot; alt=&quot;record-player on Glitch&quot; style=&quot;height: 100%; width: 100%; border: 0;&quot;&gt;
  &lt;/iframe&gt;
&lt;/div&gt;
&lt;figcaption&gt;A Glitch embed of the Record Player app&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The ‚Äòfinal‚Äô version of the project, ‚Äú&lt;a href=&quot;http://record-player.glitch.me/&quot;&gt;Record Player&lt;/a&gt;‚Äù was surprisingly simple. When given a reasonably well lit image of an even somewhat popular album the Google Vision API is able to identify the name of the album (occasionally just the artist). There were a few words I hard coded to ignore (things like ‚Äúvinyl‚Äù, ‚Äúcd‚Äù, or ‚Äúimport‚Äù), but other than that I was able to send Google Vision‚Äôs ‚Äúbest guess‚Äù to Spotify, and play the first result. I designed a goofy front end (with every music related emoji) and shared it on Twitter thinking that a few people would try it.&lt;/p&gt;
&lt;figure&gt;
&lt;div class=&quot;glitch-embed-wrap&quot; style=&quot;height: 420px; width: 100%;&quot;&gt;
  &lt;iframe allow=&quot;geolocation; microphone; camera; midi; encrypted-media&quot; src=&quot;https://glitch.com/embed/#!/embed/record-player?path=censoredWords.js&amp;previewSize=0&quot; alt=&quot;record-player on Glitch&quot; style=&quot;height: 100%; width: 100%; border: 0;&quot;&gt;
  &lt;/iframe&gt;
&lt;/div&gt;
&lt;figcaption&gt;A Glitch embed with my surprisingly short list of censored words&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;a href=&quot;https://twitter.com/anildash/&quot;&gt;Anil Dash&lt;/a&gt; unexpectedly shared, and quickly it it started showing up &lt;a href=&quot;https://pitchfork.com/news/new-app-is-basically-shazam-for-album-covers/&quot;&gt;in&lt;/a&gt; &lt;a href=&quot;https://www.pastemagazine.com/articles/2018/05/new-app-automatically-recognizes-album-covers.html&quot;&gt;a&lt;/a&gt; &lt;a href=&quot;https://www.nme.com/news/music/app-album-cover-shazam-2306795&quot;&gt;lot&lt;/a&gt; &lt;a href=&quot;https://www.engadget.com/2018/05/04/record-player-app-image-based-spotify-search/&quot;&gt;of&lt;/a&gt; &lt;a href=&quot;https://www.altpress.com/news/app_matches_album_covers_spotify/&quot;&gt;strange&lt;/a&gt; &lt;a href=&quot;https://www.androidauthority.com/record-player-spotify-google-861977/&quot;&gt;corners&lt;/a&gt; &lt;a href=&quot;https://www.rollingstone.it/musica/news-musica/ora-esiste-uno-shazam-per-le-copertine-degli-album/410773/&quot;&gt;of&lt;/a&gt; &lt;a href=&quot;https://lifehacker.com/stream-a-vinyl-album-by-snapping-a-pic-of-its-cover-art-1825800020&quot;&gt;the&lt;/a&gt; &lt;a href=&quot;https://www.thecurrent.org/feature/2018/05/02/app-album-covers&quot;&gt;internet&lt;/a&gt;. I was flattered by the coverage, but a few people were pointing out, ‚Äúwhy would you take a picture instead of just typing in the name of the album?‚Äù First of all, it‚Äôs fun, which should be reason enough. But beyond that, there‚Äôs really no reason to right now, there are too many barriers and restrictions in place in the tools we use every day like Spotify and Google, that making a mashup like this will usually require an engineer instead of just an imagination.&lt;/p&gt;
&lt;h2&gt;Getting Record Player working on a Raspberry Pi&lt;/h2&gt;
&lt;p&gt;From the beginning I wanted to create a physical version of Record Player (inspired by the amazing things they seem to be doing at &lt;a href=&quot;https://dynamicland.org/&quot;&gt;Dynamicland&lt;/a&gt; without ‚Äútraditional‚Äù input and output devices). It would be a machine that could detect when you put a record cover in front of it, and automatically start playing the first song. No screens, no searching, no curated playlist to distract from the physical thing in my hands.&lt;/p&gt;
&lt;p&gt;For the second time I was pleasantly surprised that by finding the right tools, a proof of concept was much easier than I expected. Using a Raspberry Pi with a camera module attached made a simple way to capture images. This left only the challenge of identifying when to start playing music. At first I thought that this might be easy to do by taking advantage of Google‚Äôs advanced image processing again, but I realized that the cost of the Google Vision API queries (you only get a certain amount free per month) would be prohibitive if I wanted the device to respond automatically.&lt;/p&gt;
&lt;p&gt;I decided to see if a simple algorithm running on the Raspberry Pi could identify when something new was placed in front of the camera, which seemed to work well enough. Connecting this to a slightly modified version of the Node.js server that runs the original Record Player Glitch app created exactly the machine I had imagined. The video below shows my ‚ÄúRecord Player‚Äù automatically starting playback when it sees a record cover. This prototype has screens, but they‚Äôre only used to start the app and troubleshoot.&lt;/p&gt;
&lt;iframe src=&quot;https://player.vimeo.com/video/288443309&quot; width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;A video of the Record Player machine running on a Raspberry Pi&lt;/p&gt;
&lt;p&gt;Anil‚Äôs suggestion for using ‚ÄúRecord Player‚Äù when he shared the Glitch app, was to see what song came up when you take a selfie, or ‚ÄúShazam for your face‚Äù as he put it. Of course this was hilarious, and this is the kind of play and dynamic interactions that can happen when we start to break down the strict parameters and physical barriers that currently constrain our interactions with technology.&lt;/p&gt;
&lt;p&gt;Browsing Spotify is infinitely less interesting than browsing a friend‚Äôs record collection, but the convenience of any song ever recorded will usually win over the joy of discovering something unexpected. If we want this kind of ‚Äòmagic‚Äô to bring technology like Google Vision out of prepackaged apps in our phones and laptops we need to allow for new ways of communicating with computers, and each other, that allow for experimentation, weirdness and imagination.&lt;/p&gt;
&lt;p&gt;When we start to join the interesting parts of the real world (for example: everything created before 1997) with the conveniences that smartphones have brought we can start to imagine dynamic digital experiences like the analog ones we have thought up for hundreds of years. Vinyl records themselves were designed as a static medium, but creative musicians saw a way to interact and started sampling and scratching them to create hip-hop.&lt;/p&gt;
&lt;p&gt;The idea for ‚ÄúRecord Player‚Äù only seemed obvious to me after a few hours of sifting through documentation for dozens of APIs. As users we can‚Äôt see the ‚Äúshape‚Äù of the internet, but these shapes determine what we can and can‚Äôt do, what we can and can‚Äôt imagine is possible. Surely every search on a weather or maps app is a city or address, almost all searches on Google Images are for nouns, and every search on Spotify or Genius is for an artist or song. But try connecting these services and APIs along unusual angles and you will start running into walls.&lt;/p&gt;
&lt;p&gt;Throw a song at Google Maps to see where it was recorded? Pass a Yelp page to the New York Times API to find any news about the neighborhood? These queries might be possible, but they almost always require diving into the nitty gritty of several APIs, and massaging the data to make the connection. In today‚Äôs world ideas like ‚ÄúRecord Player‚Äù have to be a million dollar business to be worth doing. Many articles written about the Record Player app implied that Spotify had made it. The way that we experience modern technology tells us that only huge corporations are able to create new ways of experiencing the world.&lt;/p&gt;
&lt;p&gt;Products like &lt;a href=&quot;https://glitch.com/&quot;&gt;Glitch&lt;/a&gt;, &lt;a href=&quot;https://www.raspberrypi.org/&quot;&gt;Raspberry Pi&lt;/a&gt;, &lt;a href=&quot;https://ifttt.com/&quot;&gt;IFTTT&lt;/a&gt;, &lt;a href=&quot;https://support.apple.com/guide/shortcuts/welcome/ios&quot;&gt;Shortcuts&lt;/a&gt;, and the inspiration for the Raspberry Pi Record Player, &lt;a href=&quot;https://dynamicland.org/&quot;&gt;Dynamicland&lt;/a&gt; make a more interesting and open future seem possible, now we just need to build the rest of it.&lt;/p&gt;
&lt;p&gt;Start now by remixing Record Player on Glitch:
&lt;a href=&quot;https://glitch.com/~record-player&quot;&gt;https://glitch.com/~record-player&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Or make your own Raspberry Pi powered version:
&lt;a href=&quot;https://github.com/patrickweaver/record-player-rpi&quot;&gt;https://github.com/patrickweaver/record-player-rpi&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>I Could Never Remember How to Make a Simple S3 Upload Feature So I Wrote It Down</title>
    <link href="https://patrickweaver.net/blog/i-could-never-remember-how-to-make-a-simple-s3-upload-feature-so-i-wrote-it-down/"/>
    <updated>2020-04-21T00:00:00-00:00</updated>
    <id>https://patrickweaver.net/blog/i-could-never-remember-how-to-make-a-simple-s3-upload-feature-so-i-wrote-it-down/</id>
    <content type="html">&lt;p&gt;Whenever I start a new web project there is an ominous, literal, figurative, &amp;quot;cloud&amp;quot; lurking on the horizon: Will this project get complicated enough to need to be connected to S3 for file upload?&lt;/p&gt;
&lt;p&gt;More often than I&#39;d like the answer is yes, and at this point I&#39;ve re-learned how to connect a Node.js app to S3 more times than I&#39;d like. Rather than keep learning just enough S3 to get a project working, and then instantly forgetting it, I decided to write the process down so I can follow my own instructions.&lt;/p&gt;
&lt;p&gt;I&#39;m sure this will also find its way to people who know more than I do and might be able to alert me to anything I&#39;m doing wrong. If this is you, &lt;a href=&quot;https://twitter.com/patrickweave_r&quot;&gt;please reach out&lt;/a&gt;!&lt;/p&gt;
&lt;h2&gt;Setting Up AWS Authentication&lt;/h2&gt;
&lt;p&gt;Connecting an app isn&#39;t usually the most difficult part of setting up S3. Where I always have to go back to documentation is setting up user and bucket permissions correctly. When I first started using S3 around 2013 a common recommendation was to just set buckets to public and link to objects directly. More recently though, many people (including Amazon), recommend not making buckets public.&lt;/p&gt;
&lt;p&gt;In my experience, it&#39;s best to create both a user and a policy when setting up AWS permissions. The keys you will use in your app will be associated with the user, and the permissions you want your user to have will be associated with the policy. This way, if your credentials are compromised you can create a new user, and all you have to do is add the policy to the new user.&lt;/p&gt;
&lt;p&gt;I&#39;ve also found it&#39;s a best practice to create a new bucket for each of the small apps that I make. If you&#39;re working on a bigger project or want to set up a general purpose place to upload you may want to do this differently, but creating a unique bucket and user for each project helps me keep an eye on things, and not worry too much about credentials getting compromised. Because I only need one bucket for my app it&#39;s easier to create it in the AWS web interface than to build functionality to create buckets into my app.&lt;/p&gt;
&lt;h4&gt;Creating a Bucket&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Log into AWS and click on &amp;quot;Services&amp;quot; in the top left. Select &amp;quot;S3&amp;quot; in the &amp;quot;Storage&amp;quot; section, then click on &amp;quot;Create Bucket&amp;quot; on the main S3 screen.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://dev-to-uploads.s3.amazonaws.com/i/wzb11or02x3fgsdlobl1.png&quot; alt=&quot;A screenshot of the main S3 screen&quot;&gt;&lt;/p&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;Give your bucket a name (this will be visible to users so something related to your app is best), and select a region (probably whichever is closest to your users), leave &amp;quot;Block all public access&amp;quot; checked, then click &amp;quot;Create bucket&amp;quot;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://dev-to-uploads.s3.amazonaws.com/i/jqobhso6ba8idwecff7l.png&quot; alt=&quot;A screenshot of the Create bucket screen&quot;&gt;&lt;/p&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;Note your bucket name (probably in an ENV variable), it&#39;s now ready to receive uploads!&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Creating a Policy&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Click on your name in the top right. In the dropdown select &amp;quot;My Security Credentials&amp;quot;, then in the &amp;quot;Identity and Access Management (IAM)&amp;quot; sidebar on the left, click on &amp;quot;Policies&amp;quot;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click on the &amp;quot;Create policy&amp;quot; button. There are 2 ways to give your policy permissions, with the Visual Editor, and with JSON. We&#39;ll use the Visual Editor here, but you can probably just pate the JSON at the end with minor edits.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The Visual Editor has 4 sections: Service, Actions, Resources, and Request Conditions. Start in Service and click on S3.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You want to add 3 specific actions: &amp;quot;PutObject&amp;quot; which allows uploading files, &amp;quot;GetObject&amp;quot; which allows reading files, and &amp;quot;DeleteObject&amp;quot; (I think you can figure this one out). &amp;quot;GetObject&amp;quot; is in the &amp;quot;Read&amp;quot; section, check the checkbox there. &amp;quot;PutObject&amp;quot; and &amp;quot;DeleteObject&amp;quot; are both in the &amp;quot;Write&amp;quot; section. At the end you should have 3 objects selected:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://dev-to-uploads.s3.amazonaws.com/i/xq9fza7qlzaxb9s2uwxx.png&quot; alt=&quot;A screenshot of the Create Policy actions selection&quot;&gt;&lt;/p&gt;
&lt;ol start=&quot;5&quot;&gt;
&lt;li&gt;In the Resources section click on &amp;quot;Add ARN&amp;quot;, then fill in your Bucket Name, and click on &amp;quot;Any&amp;quot; for Object name. This means that users with this policy can only perform the actions above on one bucket, but can perform those actions on any of the objects in that bucket.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://dev-to-uploads.s3.amazonaws.com/i/n99mhoauau96yg2fp6j2.png&quot; alt=&quot;A screenshot of the Add ARN screen when creating a policy&quot;&gt;&lt;/p&gt;
&lt;ol start=&quot;6&quot;&gt;
&lt;li&gt;If you click over to the JSON editor you should see the code below. You can also just copy this in. Note that you should edit the &amp;quot;Resource&amp;quot; property to have your actual bucket name:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{
    &amp;quot;Version&amp;quot;: &amp;quot;2012-10-17&amp;quot;,
    &amp;quot;Statement&amp;quot;: [
        {
            &amp;quot;Sid&amp;quot;: &amp;quot;VisualEditor0&amp;quot;,
            &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;,
            &amp;quot;Action&amp;quot;: [
                &amp;quot;s3:PutObject&amp;quot;,
                &amp;quot;s3:GetObject&amp;quot;,
                &amp;quot;s3:DeleteObject&amp;quot;
            ],
            &amp;quot;Resource&amp;quot;: &amp;quot;arn:aws:s3:::YOUR_BUCKET_NAME/*&amp;quot;
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&quot;6&quot;&gt;
&lt;li&gt;Click on &amp;quot;Review policy&amp;quot;, then give your policy a name and a description. Then click &amp;quot;Create policy&amp;quot;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Creating a User&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Click on Users in the left sidebar, then the &amp;quot;Add user&amp;quot; button at the top of the screen, give your user a name and select the checkbox for &amp;quot;Programmatic Access&amp;quot;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://dev-to-uploads.s3.amazonaws.com/i/gi90azo1auvxzjdbamyz.png&quot; alt=&quot;A screenshot of the Add User screen&quot;&gt;&lt;/p&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;
&lt;p&gt;In the &amp;quot;Set permissions&amp;quot; section at the top of the page, click on &amp;quot;Attach existing policies directly&amp;quot;. Search for the policy you just created, then select it and click &amp;quot;Next: Tags&amp;quot;. You can skip Tags, and click &amp;quot;Next: Review&amp;quot;, then click &amp;quot;Create user&amp;quot;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You will now save your user&#39;s credentials. This is the only time you will be able to do this, so make sure you save them somewhere safe. You will also need to add the credentials as ENV variables in your app. I recommend clicking the &amp;quot;Download .csv&amp;quot; button and saving the file, at least until you get your app set up.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://dev-to-uploads.s3.amazonaws.com/i/3rqbznl2dlvif555eorn.png&quot; alt=&quot;A screenshot of the attach policy section of the create user screen&quot;&gt;&lt;/p&gt;
&lt;h2&gt;A simple example app&lt;/h2&gt;
&lt;p&gt;Congratulations! You are done with the AWS setup, now you can work on your app. I have a simple and heavily commented &lt;a href=&quot;https://aws-s3-example.glitch.me/&quot;&gt;example app&lt;/a&gt; I use to add this functionality to new projects:&lt;/p&gt;
&lt;!-- Copy and Paste Me --&gt;
&lt;div class=&quot;glitch-embed-wrap&quot; style=&quot;height: 420px; width: 100%;&quot;&gt;
  &lt;iframe src=&quot;https://glitch.com/embed/#!/embed/aws-s3-example?path=package.json&amp;previewSize=100&quot; title=&quot;aws-s3-example on Glitch&quot; allow=&quot;geolocation; microphone; camera; midi; vr; encrypted-media&quot; style=&quot;height: 100%; width: 100%; border: 0;&quot;&gt;
  &lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;The app is a Node.js app using Express. It uses 3 additional packages. &lt;a href=&quot;https://patrickweaver.net/blog/i-could-never-remember-how-to-make-a-simple-s3-upload-feature-so-i-wrote-it-down/aws-sdk&quot;&gt;https://www.npmjs.com/package/aws-sdk&lt;/a&gt; adds functionality to communicate with S3, &lt;a href=&quot;https://patrickweaver.net/blog/i-could-never-remember-how-to-make-a-simple-s3-upload-feature-so-i-wrote-it-down/uuid&quot;&gt;https://www.npmjs.com/package/uuid&lt;/a&gt; is used for object names in S3, and [https://www.npmjs.com/package/multer]multer is used to process file upload to the server before passing it to S3.&lt;/p&gt;
&lt;p&gt;The index page is a plain HTML file, but there are two POST routes in server.js: &lt;code&gt;/upload-image-form&lt;/code&gt; and &lt;code&gt;/upload-image-async&lt;/code&gt;. The two routes are mostly the same, but are repeated for easy copying.&lt;/p&gt;
&lt;p&gt;Lines 1 through 24 of server.js are setting up the dependencies:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;server.js&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;// The regular Node/Express stuff:
const express = require(&#39;express&#39;);
const app = express();
app.use(express.static(&#39;public&#39;));

// I will use the UUID package for s3 file names
const { v4: uuidv4 } = require(&#39;uuid&#39;);

// The AWS functionality is isolated for clarity:
const aws = require(&#39;./aws.js&#39;);

// Multer processes the file in the request body
// This allows one file to be uploaded at a time.
var multer = require(&#39;multer&#39;);

var memoryStorage = multer.memoryStorage();
var memoryUpload = multer({
	storage: memoryStorage,
	limits: {
		fileSize: 4*1024, // 4KB filesize limit
    //fileSize: 10*1024*1024, // 10 Mb filesize limit
		files: 1
	}
}).single(&#39;file&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The uploading to S3 happens in the two POST routes, and in an isolated &lt;code&gt;aws&lt;/code&gt; module. I will go through the regular HTML form route here, but the JS API endpoint route is mostly the same.&lt;/p&gt;
&lt;p&gt;The route uses the previously defined &lt;code&gt;memoryUpload&lt;/code&gt; to capture a file object in req.body.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;server.js&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;app.post(&#39;/upload-image-form&#39;, memoryUpload, async function(req, res) {
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we create an object to send to the &lt;code&gt;aws&lt;/code&gt; module (this is custom to this app, not the &lt;code&gt;aws-sdk&lt;/code&gt; npm package) with req.file. Most of the code below is comments, but the short version of what we need to send to the aws is an object with the properties &lt;code&gt;file&lt;/code&gt; and &lt;code&gt;id&lt;/code&gt;. &lt;code&gt;file&lt;/code&gt; is the contents of the file, &lt;code&gt;id&lt;/code&gt; is what the file will be called in our AWS bucket:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;server.js&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;    const file = req.file;
    
    /*
    The file object has the following fields:
    
    fieldname: &#39;file&#39; // This was specified in the file input field in the HTML
    originalname:     // The original name of the file
    encoding:         // The encoding of the file, don&#39;t worry about
                         this unless you want to look at the bytes.
    mimetype:         // This will tell you what the filetype is, even if there
                         is no extension, or if it&#39;s wrong.
    buffer:           // This is the actual data from the file
    size:             // Only some files will have this, the file&#39;s size in bytes
    */
    
    
    // This is optional, but a way to find the extension
    // of an image file.
    //const fileExt = file.mimetype.split(&amp;quot;/&amp;quot;);

    // These
    const upload = {
      file: file,
      
      /* You may want to store this metadata in S3, but it&#39;s optional */
      filetype: file.mimetype,
      
      /* You may want to add this to the filename */
      //fileExt: fileExt[fileExt.length - 1],
      
      /* You may want to use the original filename */
      //filename: file.originalname,
      
      /* We&#39;re going to use a random UUID file name in this example.
         One thing that this does is makes sure it is unique.
         If you upload a file with the same name it will overwrite the
         existing file! */
      id: uuidv4()
    }
  
    // Upload the file, see ./helpers/aws.js
    const response = await aws.upload(upload);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the &lt;code&gt;aws.js&lt;/code&gt; module first there is some general configuration. This is where we will access our &lt;code&gt;AWS_ACCESS_KEY_ID&lt;/code&gt;, &lt;code&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt;, and &lt;code&gt;S3BUCKET&lt;/code&gt; ENV variables.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;aws.js&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;// The AWS package is used for all AWS services,
// we only need the S3 part:
var AWS = require(&#39;aws-sdk&#39;);
var s3 = new AWS.S3({
  signatureVersion: &#39;v4&#39;
});

// Store your AWS creds in ENV variables:
AWS.config.update({
    accessKeyId: process.env.AWS_ACCESS_KEY_ID,
    secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY
});

// Your bucket isn&#39;t secret, but you may want to use
// different buckets for dev and production so it&#39;s
// helpful to store in an ENV variable.
var bucketName = process.env.S3BUCKET;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are also 2 functions: &lt;code&gt;upload()&lt;/code&gt;, which takes one &lt;code&gt;uploadObject()&lt;/code&gt; parameter, uploads a file to S3, and returns confirmation and the S3 object&#39;s key, and &lt;code&gt;getSignedUrl&lt;/code&gt;, which takes an S3 key, and returns the file (more on this later).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;upload()&lt;/code&gt; is what we passed our &lt;code&gt;file&lt;/code&gt; object from &lt;code&gt;server.js&lt;/code&gt; to. This function is essentially a wrapper around the &lt;code&gt;aws-sdk&lt;/code&gt;&#39;s &lt;code&gt;S3.putObject()&lt;/code&gt; method. We collect the necessary parameters in an object, then pass that object to the method which we&#39;ve defined as &lt;code&gt;s3.putObject()&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;aws.js&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;    // AWS S3 Upload params:
    var params = {
      // S3 stores files in buckets, each bucket
      // has a globally unique name.
      Bucket: bucketName,

      // This will be the filename in AWS
      Key: uploadObject.id,

      // This is the contents of the file.
      Body: uploadObject.file.buffer,

      // This is optional, but your file in S3 won&#39;t have Content-Type
      // metadata unless you include it.
      ContentType: uploadObject.filetype
    };
  
  
    const responseData = await s3.putObject(params).promise();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is all wrapped in a &lt;code&gt;try&lt;/code&gt; / &lt;code&gt;catch&lt;/code&gt; block so if there aren&#39;t any errors we can pass the key back to &lt;code&gt;server.js&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;aws.js&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;    // Likely this won&#39;t happen because an error will be thrown,
    // but it&#39;s good to check just in case. ¬Ø\_(„ÉÑ)_/¬Ø 
    if (!responseData) {
      throw &amp;quot;Upload failed&amp;quot;
    }
      
    // The response data has a single property, &amp;quot;ETag&amp;quot;,
    // you probably won&#39;t need to do anything with it.

    const s3Data = {
      success: true,

      // This key is what you would store in a DB, we didn&#39;t
      // get this back from S3, but since there wasn&#39;t an error
      // we trust that it is saved.
      key: params.Key

      // Or, the url below could be stored if the permissions on the bucket
      // or the upload are publically viewable.
      //url: &amp;quot;https://&amp;quot; + bucketName + &amp;quot;.s3.amazonaws.com/&amp;quot; + params.Key
    }

    // Send the object with success and the key back to server.js
    return(s3Data)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It&#39;s important to note that the &lt;code&gt;id&lt;/code&gt; we pass back to &lt;code&gt;server.js&lt;/code&gt; isn&#39;t returned to us from the &lt;code&gt;s3.putObject()&lt;/code&gt; method. &lt;code&gt;s3()&lt;/code&gt; returns an &lt;code&gt;ETag&lt;/code&gt;, which isn&#39;t of much use for what we&#39;re doing, but it&#39;s enough to confirm that the upload completed successfully (What are ETags? &lt;a href=&quot;https://teppen.io/2018/06/23/aws_s3_etags/&quot;&gt;teppen.io/2018/06/23/aws_s3_etags/&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Going back to server.js, this is where we would want to store our &lt;code&gt;id&lt;/code&gt; somewhere. This string is what we will need to retrieve the file from s3. In this app we&#39;re just demoing the upload functionality so we don&#39;t store it anywhere. We access it once though to show the user that it worked. This is where we will use the other function in the &lt;code&gt;aws&lt;/code&gt; module, &lt;code&gt;getSignedUrl&lt;/code&gt;. Because our S3 bucket permissions only let our AWS user access objects, and otherwise our bucket permissions are &amp;quot;No public access&amp;quot;, we need to create a temporary signed URL to access the file.&lt;/p&gt;
&lt;p&gt;Using the id returned from the &lt;code&gt;upload()&lt;/code&gt; function we call the &lt;code&gt;getSignedUrl()&lt;/code&gt; function. When we get the signed url, we put it into some simple HTML to display it to the user (this is the main difference between the two &lt;code&gt;server.js&lt;/code&gt; routes):&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;server.js&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;    // Confirm upload succeeded:
    if (!response.success || response.error) {
      throw &amp;quot;Reponse Error: &amp;quot; + response.error;
    }
    
    /* - - - - -
      You might want to do something with the response.key or
      response.url here.
    - - - - - */
    
    
    // Because our bucket is not publically viewable we need to
    // get a signed URL to view the uploaded file. You DO NOT want
    // to store this signed URL in a DB, it will expire. You will
    // want to store either the key or url from the AWS response
    // above.
    
    // Get a new signed URL now that the file is uploaded:
    // Getting a signed URL requires the Bucket Name and the
    // file id, but we are using the same bucket name for everything
    // in this example. See ./helpers/aws.js for how this works.
    const url = await aws.getSignedUrl(upload.id);

    // Very simple HTML response containing the URL and it rendered
    // as an image (if the file is not an image this will look like
    // a broken image).
    res.status(200).send(`
      &amp;lt;p&amp;gt;
        &amp;lt;strong&amp;gt;Signed URL:&amp;lt;/strong&amp;gt; &amp;lt;a href=&amp;quot;${url}&amp;quot;&amp;gt;${url}&amp;lt;/a&amp;gt;
      &amp;lt;/p&amp;gt;
      &amp;lt;h4&amp;gt;If it&#39;s an image:&amp;lt;/h4&amp;gt;
      &amp;lt;img src=&amp;quot;${url}&amp;quot; width=&amp;quot;400&amp;quot; /&amp;gt;
    `); 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;getSignedUrl()&lt;/code&gt; function in &lt;code&gt;aws&lt;/code&gt; is a wrapper around the &lt;code&gt;S3.getSignedUrl&lt;/code&gt; method (mostly putting it in our &lt;code&gt;aws&lt;/code&gt; module allows us to avoid passing the Bucket Name from our routes:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;aws.js&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;// This function will get a signed URL which allows
// access to non public objects, and objects in non
// public buckets for a limited time.
async function getSignedUrl(key) {
  
  // We are already authenticated so we just need the
  // bucket name and the object&#39;s key.
  var params = {
    Bucket: bucketName,
    Key: key
  };
  
  // The getSignedUrl method returns the url.
  const url = await s3.getSignedUrl(&#39;getObject&#39;, params);
  return url
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That&#39;s it! Try out the app (in this example uploads are limited in size to 4KB for safety). You can &lt;a href=&quot;https://glitch.com/edit/#!/aws-s3-example&quot;&gt;remix the app on Glitch&lt;/a&gt; or &lt;a href=&quot;https://github.com/patrickweaver/aws-s3-example&quot;&gt;fork it on GitHub&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>How to Download an Image from a Google Doc</title>
    <link href="https://patrickweaver.net/blog/how-to-download-an-image-from-a-google-doc/"/>
    <updated>2020-07-16T00:00:00-00:00</updated>
    <id>https://patrickweaver.net/blog/how-to-download-an-image-from-a-google-doc/</id>
    <content type="html">&lt;p&gt;For some reason Google hasn&#39;t built in a way for you to download images in Google docs! There are workarounds to get those image files like &lt;a href=&quot;https://twitter.com/corduroy/status/1184758335934849025&quot;&gt;using Google Keep&lt;/a&gt;, or &lt;a href=&quot;https://twitter.com/tonyvincent/status/1021726699178708993&quot;&gt;downloading your whole doc as a .zip file&lt;/a&gt;, but these have always felt like too many steps.&lt;/p&gt;
&lt;p&gt;And this is something that people really want!&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;a href=&quot;https://twitter.com/user/status/1190182191520788480&quot;&gt;&lt;/a&gt;&lt;/blockquote&gt; &lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;a href=&quot;https://twitter.com/user/status/1277776054380265478&quot;&gt;&lt;/a&gt;&lt;/blockquote&gt; &lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;a href=&quot;https://twitter.com/user/status/710516705303384068&quot;&gt;&lt;/a&gt;&lt;/blockquote&gt; &lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;a href=&quot;https://twitter.com/user/status/1227582581350240257&quot;&gt;&lt;/a&gt;&lt;/blockquote&gt; &lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;a href=&quot;https://twitter.com/user/status/1225516004375179265&quot;&gt;&lt;/a&gt;&lt;/blockquote&gt; &lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;a href=&quot;https://twitter.com/user/status/1249761603559378945&quot;&gt;&lt;/a&gt;&lt;/blockquote&gt; &lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;a href=&quot;https://twitter.com/user/status/990395429383622656&quot;&gt;&lt;/a&gt;&lt;/blockquote&gt; &lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;p&gt;As Steve Krouse points out here, it is possible to get the real URL of the image in your doc (but confusingly, as soon as you click on the image to select it the URL becomes obfuscated!).&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;a href=&quot;https://twitter.com/user/status/1190358282877186050&quot;&gt;&lt;/a&gt;&lt;/blockquote&gt; &lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;p&gt;I also noticed the URLs in the source, and decided to make an easy way to access it. The one trick ended up being, because clicking on the image made it disappear, finding a way to tell the code which image you wanted!&lt;/p&gt;
&lt;p&gt;I looked through some JavaScript documentation and realized I could use the &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/Element/mouseover_event&quot;&gt;mouseover&lt;/a&gt; event to detect when someone was hovering over the image. Unfortunately this means that it won&#39;t work on a touchscreen device, but I&#39;m guessing that most people who want to download an image are on a traditional computer.&lt;/p&gt;
&lt;h2&gt;How does it work?&lt;/h2&gt;
&lt;p&gt;I needed a way to run my code on any Google Doc, there&#39;s probably a way to make Google Doc or Chrome extension to do this, but since I was asking people to run code in their potentially private docs I wanted to make the code as short and open source as possible.&lt;/p&gt;
&lt;p&gt;I decided that the best way to do this was a &lt;a href=&quot;https://support.mozilla.org/en-US/kb/bookmarklets-perform-common-web-page-tasks&quot;&gt;bookmarklet&lt;/a&gt;. If you&#39;re unfamiliar with bookmarklets, they&#39;re bookmarks (usually placed in your bookmarks toolbar (Cmd-Shift-B to toggle this on and off on a Mac), that instead of navigating to a webpage, run JavaScript when you click them.&lt;/p&gt;
&lt;h2&gt;Great! Tell me how to do it!&lt;/h2&gt;
&lt;p&gt;To get started you&#39;ll have to &amp;quot;install&amp;quot; the bookmarklet. This is easy to do, and just means dragging a button into your bookmarks toolbar. &lt;a href=&quot;https://gdoc-image-dl.glitch.me/&quot;&gt;I&#39;ve hosted it on Glitch here&lt;/a&gt;. You can even drag it straight from one of the buttons on the embed below:&lt;/p&gt;
&lt;div class=&quot;glitch-embed-wrap&quot; style=&quot;height: 420px; width: 100%;&quot;&gt;&lt;iframe allow=&quot;geolocation; microphone; camera; midi; encrypted-media&quot; src=&quot;https://glitch.com/embed/#!/embed/gdoc-image-dl?path=README.md&amp;previewSize=100&quot; alt=&quot;record-player on Glitch&quot; style=&quot;height: 100%; width: 100%; border: 0;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;p&gt;The instructions are simple!&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Drag one of the bookmarklets below to your bookmarks toolbar. The text displayed is what will be show on the toolbar:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;üñº‚§µÔ∏è GDoc Image DL
2. Then, when you&#39;re on a Google Doc, click the bookmarklet, then hover over an image embeded in the doc. Depending on your browser settings it will either download immediately, or open the actual image in a new tab.&lt;/p&gt;
&lt;h2&gt;Great my problems are solved forever!&lt;/h2&gt;
&lt;p&gt;No guarantees that this will work long term, a quick look at the source code for any Google Doc will show that they&#39;re very complex! I wouldn&#39;t be surprised if Google changes the way these URLs work in the future, but this tool has worked for 6 months so maybe not!&lt;/p&gt;
&lt;p&gt;Long term I hope that they build in a way for people to download their images, but for now I hope this is helpful!&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>Week 1 at Recurse Center: Two Approaches to Learning</title>
    <link href="https://patrickweaver.net/blog/recurse-center-week-1/"/>
    <updated>2020-08-16T22:22:13.781-00:00</updated>
    <id>https://patrickweaver.net/blog/recurse-center-week-1/</id>
    <content type="html">&lt;p&gt;I just finished my first week as part of the Fall 1 &#39;20 batch at &lt;a href=&quot;https://www.recurse.com/&quot;&gt;Recurse Center&lt;/a&gt;. I tried to split my time between being social and building skills that I want to use for future projects. The social aspect of RC is interesting because this batch is being conducted remotely.&lt;/p&gt;
&lt;p&gt;My batch at RC started exactly 5 months after my last day in an office, and something I had been thinking about over the last few months, is that I haven&#39;t met anyone new since early March when NYC shut down because of the pandemic. It&#39;s been very refreshing to meet people again, even if it&#39;s via video calls and chat. RC has created an online representation of their physical space we call &amp;quot;Virtual RC&amp;quot;. Each of us have avatars we can move around the space, and there are permanent links to video call rooms that we can pop into for events or impromptu conversations. The Virtual RC experience pairs well with group chat organized into streams on different topics.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;https://patrickweaver.net/images/blog/rc/virtual-rc.png&quot; alt=&quot;A screenshot of Virtual RC&quot;&gt;&lt;/p&gt;
&lt;figcaption&gt;My avatar hanging out in the &quot;Shannon&quot; room at Virtual RC&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Other than social events I spent my first week at RC focused on two projects. The first was going through the &lt;a href=&quot;https://www.hackingwithswift.com/100/swiftui&quot;&gt;100 Days of SwiftUI tutorials&lt;/a&gt; (at many more than one a day), which I had started before the batch. The second was reviewing the &lt;a href=&quot;https://github.com/patrickweaver/nyc-subway&quot;&gt;real time NYC Subway map&lt;/a&gt; project I first started in January.&lt;/p&gt;
&lt;p&gt;I wish I had been able to finish the SwiftUI tutorials before the batch (my original goal), because it didn&#39;t feel as useful to share what I was working on with other RC participants as I was re-building apps that someone else had designed, but I finished the last app on Friday and I&#39;m excited to be able to use SwiftUI to quickly prototype projects for my phone.&lt;/p&gt;
&lt;p&gt;When I first started the subway map project in early 2020 I was excited about finishing it quickly, but in March when I also stopped taking the subway I lost enthusiasm for the project. Starting at RC has helped rekindle the excitement I had for it, I think partly because even though the batch is remote and there are participants from around the world, there is also a strong NYC contingent.&lt;/p&gt;
&lt;p&gt;An interesting aspect of building a map from MTA data is that the data structures that the MTA publishes are designed for building countdown clock style apps. I would imagine that the MTA calculates this per-station data from the location of each train, but in order to build an app that is more focused on location than time, I need to reverse engineer the location data from the distance in time each train from the next station. There are also other weird quirks, like train trip start times being provided in &amp;quot;HH:MM:SS&amp;quot; format, but station arrival times being provided in Unix time format.&lt;/p&gt;
&lt;figure&gt;
&lt;pre&gt;&lt;code&gt;{ &amp;quot;id&amp;quot;: &amp;quot;000001G&amp;quot;,
  &amp;quot;tripUpdate&amp;quot;: {
    &amp;quot;trip&amp;quot;: {
      &amp;quot;tripId&amp;quot;: &amp;quot;126481_G..N&amp;quot;,
      &amp;quot;startTime&amp;quot;: &amp;quot;21:04:49&amp;quot;,
      &amp;quot;startDate&amp;quot;: &amp;quot;20200815&amp;quot;,
      &amp;quot;routeId&amp;quot;: &amp;quot;G&amp;quot;
    },
    &amp;quot;stopTimeUpdate&amp;quot;: [
      { &amp;quot;arrival&amp;quot;: { &amp;quot;time&amp;quot;: &amp;quot;1597541836&amp;quot; },
        &amp;quot;departure&amp;quot;: { &amp;quot;time&amp;quot;: &amp;quot;1597541836&amp;quot; },
        &amp;quot;stopId&amp;quot;: &amp;quot;G28N&amp;quot; },
      { &amp;quot;arrival&amp;quot;: { &amp;quot;time&amp;quot;: &amp;quot;1597541904&amp;quot; },
        &amp;quot;departure&amp;quot;: { &amp;quot;time&amp;quot;: &amp;quot;1597541904&amp;quot; },
        &amp;quot;stopId&amp;quot;: &amp;quot;G26N&amp;quot; }
        
      /* More stations below in real data */
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;figcaption&gt;Example MTA Data&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Splitting my programming time between learning SwiftUI and trying to wrap my head around the subway map again has felt like two different approaches to learning. In the past I have often jumped into building my own projects with a new tool or technology as soon as I feel like I had learned enough to do so, but I&#39;ve often realized later that I spent a lot of time figuring out answers that I might have gotten to more quickly by completing a more thorough overview before starting.&lt;/p&gt;
&lt;p&gt;The subway map is one example of that type of project, but for good reason. It&#39;s a project that when I started, I wasn&#39;t sure was possible. It was inspired by a similar (but much simpler) map in the underground MUNI stations in San Francisco. The MUNI maps I remember from 15 years ago only showed trains in &amp;quot;the tunnel,&amp;quot; which although many lines run through, has a single trunk, that lines branch out from after going above ground (newer maps as seen below seem to show the whole system). Since these MUNI maps had existed since at least the early 00s I figured if one hadn&#39;t been made (I&#39;ve since found &lt;a href=&quot;https://tracker.geops.ch/?z=13&amp;amp;s=1&amp;amp;x=-8232001.0970&amp;amp;y=4969606.7622&amp;amp;l=transport&quot;&gt;NYC maps that have been made&lt;/a&gt;) for NY there must be a technical reason (it may just be that because until relatively recently &lt;a href=&quot;https://www.theatlantic.com/technology/archive/2015/11/why-dont-we-know-where-all-the-trains-are/415152/&quot;&gt;per station data wasn&#39;t available&lt;/a&gt; in NYC).&lt;/p&gt;
&lt;figure&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-conversation=&quot;none&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;&lt;a href=&quot;https://twitter.com/MBTA?ref_src=twsrc%5Etfw&quot;&gt;@MBTA&lt;/a&gt; &lt;a href=&quot;https://twitter.com/sfmta_muni?ref_src=twsrc%5Etfw&quot;&gt;@sfmta_muni&lt;/a&gt; &lt;a href=&quot;https://twitter.com/d_tribe?ref_src=twsrc%5Etfw&quot;&gt;@d_tribe&lt;/a&gt; &lt;a href=&quot;https://twitter.com/universalhub?ref_src=twsrc%5Etfw&quot;&gt;@universalhub&lt;/a&gt; closer zoom. Incredibly useful. &lt;a href=&quot;https://t.co/jqHuGyZrJR&quot;&gt;pic.twitter.com/jqHuGyZrJR&lt;/a&gt;&lt;/p&gt;&amp;mdash; Ari Ofsevit (@ofsevit) &lt;a href=&quot;https://twitter.com/ofsevit/status/720301082899918850?ref_src=twsrc%5Etfw&quot;&gt;April 13, 2016&lt;/a&gt;&lt;/blockquote&gt; &lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;figcaption&gt;
&lt;p&gt;A photo of the real time MUNI map in a tweet by &lt;a href=&quot;https://twitter.com/ofsevit/status/720301082899918850&quot;&gt;@ofsevit&lt;/a&gt;&lt;/p&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;In order to gauge the feasibility of the project I wanted to get started quickly, throwing a map together and adding some markers for stations and individual trains at fixed times. Because I had limited experience working with transit data (which is provided in a &lt;a href=&quot;https://developers.google.com/transit/gtfs&quot;&gt;very complex format&lt;/a&gt;), or maps, this left me with both the sense that the project is possible, and a big spaghetti code mess.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;https://patrickweaver.net/images/blog/rc/nyc-subway-v1.png&quot; alt=&quot;A screenshot of the first prototype of my subway map app&quot;&gt;&lt;/p&gt;
&lt;figcaption&gt;The first prototype of the subway map app&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;I started looking into SwiftUI because I&#39;m interested in making my own home screen widgets once I update my phone to iOS 14, and these widgets are built using the framework. Knowing I would have time to experiment during my batch at RC, and wanting to avoid my usual new tool mess phenomenon, I prioritized giving myself a good understanding of how SwiftUI works, what it can and can&#39;t do, and a general review of Swift. Once my batch started I found it more and more difficult to keep going through the tutorials instead of jumping into an original idea, but throughout the week, spending time figuring out what I was even thinking reviewing the code for the subway map project re-motivated me to get through it.&lt;/p&gt;
&lt;p&gt;In the first week at RC the return to a daily routine has also put me back in the frame of mind I spent a lot of my last job in, where I would come up with ideas for about 3 apps a day that would solve tiny problems. One example is, after a particular session of an RC event for pair programming on software job interview style questions where we selected a problem that ended up being very difficult in the language we chose to work in, I thought that maybe I should create a mini app for the group where we could rate problems for each other. A lot of these ideas that are generated through trying to solve small problems in my routine end up being apps that organize data into text boxes, which are often not very interesting, so I&#39;m conflicted on whether or not to spend time following these threads that will likely continue to appear.&lt;/p&gt;
&lt;p&gt;A few other things that are slightly more interesting, or at least broadly useful that I want to get done during my time at RC are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The real time subway map&lt;/li&gt;
&lt;li&gt;A simple checklist app with a home screen widget made with SwiftUI&lt;/li&gt;
&lt;li&gt;A tool to convert an export from a Blogger blog to a static site generator&lt;/li&gt;
&lt;li&gt;A prototype of a self-hosted social network profile that can interface with other people&#39;s self-hosted profiles&lt;/li&gt;
&lt;li&gt;A second iteration of my &lt;a href=&quot;https://github.com/patrickweaver/ocr-email&quot;&gt;handwritten email sending project&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Something that takes ideas similar to Dynamicland (or the &lt;a href=&quot;https://www.recurse.com/blog/132-living-room-making-rc-programmable&quot;&gt;Living Room&lt;/a&gt; project at RC) into the remote world we&#39;re living in.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Overall I&#39;m very excited to be spending time at RC, and I hope that it gives me time to explore ideas with weird corners and not settle for solving small simple problems quickly.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>Making a Real-Time NYC Subway Map with Real Weird NYC Subway Data</title>
    <link href="https://patrickweaver.net/blog/making-a-real-time-nyc-subway-map-with-real-weird-nyc-subway-data/"/>
    <updated>2020-10-21T00:00:00-00:00</updated>
    <id>https://patrickweaver.net/blog/making-a-real-time-nyc-subway-map-with-real-weird-nyc-subway-data/</id>
    <content type="html">&lt;p&gt;Earlier this week the NYC MTA released a new &lt;a href=&quot;https://map.mta.info/&quot;&gt;digital-first map&lt;/a&gt;. The &lt;a href=&quot;https://www.curbed.com/2020/10/first-look-new-yorks-digital-subway-map-comes-alive-today.html&quot;&gt;Curbed exclusive&lt;/a&gt; that announced its release accurately portrays it as a strange child of both the 1972 map design by Massimo Vignelli and the current &lt;a href=&quot;https://new.mta.info/map/5256&quot;&gt;‚Äúpaper‚Äù map&lt;/a&gt;. One feature of the new map (though it&#39;s harder than it should be to notice at first) is real-time visualizations of each train in the system.&lt;/p&gt;
&lt;p&gt;I&#39;ve been working on a similar concept, starting in February 2020, on which progress stalled once I stopped riding the subway regularly in March. But, when I started my batch at &lt;a href=&quot;https://www.recurse.com/&quot;&gt;Recurse Center&lt;/a&gt; I decided to pick up the project again. My inspiration for the map was the large TV screens that the MTA has installed in stations over the last few years, which frustratingly display the ‚Äúpaper‚Äù version of the map.&lt;/p&gt;
&lt;figure&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;img src=&quot;https://patrickweaver.net/images/blog/nyc-subway/tv-screen-map.jpg&quot; alt=&quot;A photograph of a TV in a subway station with the ‚Äúpaper‚Äù map displayed.&quot; style=&quot;max-height: 400px; margin: 0 auto;&quot;&gt;
&lt;/div&gt;
&lt;figcaption&gt;Subway station TV (This is not a good photo, but it‚Äôs hard to take a picture of a screen underground)&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Over the past few weeks at RC the subway map has been my main focus, which is longer than I expected the project to take (and though I have a prototype, I wouldn‚Äôt say I‚Äôm close to ‚Äúdone‚Äù). A big factor in the time the project has taken is some of the quirks in working with MTA data, which based on some of the bugs I&#39;ve seen in the &amp;quot;official&amp;quot; version I&#39;d say the team working on that version had to grapple with as well.  I&#39;m not sure I will ever finish this project to a state that would look great on a subway station tv, or be useful, but I do want to point out &lt;a href=&quot;https://www.theweekendest.com/trains&quot;&gt;The Weekendest&lt;/a&gt; by &lt;a href=&quot;https://sunny.ng/&quot;&gt;Sunny Ng&lt;/a&gt; (who also made &lt;a href=&quot;https://www.goodservice.io/&quot;&gt;goodservice.io&lt;/a&gt;), which is a great take on the concept, and handles some of the challenges of this kind of project much better than the MTA map does.&lt;/p&gt;
&lt;p&gt;For a long time the NYC Subway was almost &lt;a href=&quot;https://www.theatlantic.com/technology/archive/2015/11/why-dont-we-know-where-all-the-trains-are/415152/&quot;&gt;completely lacking in real-time data&lt;/a&gt;. For many years the only line that had even countdown clocks in stations was the L, which seems to be the line the MTA tries out new technology on, likely because it never shares tracks with any other line. Over the last 5 years the MTA has slowly installed countdown clocks in every station, and made the data that powers the countdown clocks available on their website, in apps, and as data online.&lt;/p&gt;
&lt;p&gt;Inspired and frustrated by the ‚Äúpaper‚Äù maps on the tv screens, I first became interested in working with MTA data in 2018, but I initially started working with bus data, I think for two reasons: the first was because at the time the API key for working with bus data was easier to obtain than for subway data, the second because my morning commute at the time usually started with the bus (The MTA, earlier this year, has fortunately updated the system for obtaining an API key for subway data). I made a small prototype of an iPhone app that would show real-time bus data, but got distracted by learning the Swift programming language and abandoned the project without building functionality beyond what the MTA already provided &lt;a href=&quot;https://bustime.mta.info/&quot;&gt;on their bustime website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;That MTA Bustime website was the other inspiration for what became my map idea. Though you could only view one route at a time (and the functionality is not available on phone-size devices), the Bustime website showed, in addition to countdowns for each stop, the physical location of each bus on a map. This leads us to the first weird thing about working with NYC Subway data, unlike real-time bus data, the subway data does not contain the latitude and longitude data for each train that would make it easy to show them on a map.&lt;/p&gt;
&lt;h3&gt;Real-Time Transit Data&lt;/h3&gt;
&lt;p&gt;Transit data for most transit systems is available in formats called &lt;a href=&quot;https://developers.google.com/transit/gtfs&quot;&gt;GTFS&lt;/a&gt; (General Transit Feed Specification) and &lt;a href=&quot;https://developers.google.com/transit/gtfs-realtime&quot;&gt;GTFS Realtime&lt;/a&gt;, which were developed by Google (makes you wonder what the ‚ÄúG‚Äù originally stood for), but are now widely used. A GTFS file is, ‚Äúa collection of at least six, and up to 13 CSV files (with extension .txt) contained within a .zip file.‚Äù and a GTFS Realtime feed is ‚ÄúThe GTFS Realtime data exchange format is based on Protocol Buffers‚Äù (which are &lt;a href=&quot;https://developers.google.com/protocol-buffers&quot;&gt;‚ÄúGoogle&#39;s language-neutral, platform-neutral, extensible mechanism for serializing structured data‚Äù&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The GTFS Realtime feeds are available through 9 different API endpoints from the MTA. These 9 endpoints roughly correspond to the line colors, with Shuttles combined with trains they share tracks or stations with, and the 1/2/3 and 4/5/6 sharing one endpoint. This list of separate endpoints is another challenge with working with the entirety of the MTA data.&lt;/p&gt;
&lt;p&gt;I have exclusively worked with the NYC MTA‚Äôs GTFS Realtime feeds through the &lt;a href=&quot;https://www.npmjs.com/package/gtfs-realtime-bindings&quot;&gt;npm module&lt;/a&gt; maintained by Google. It is very possible that some of the challenges I‚Äôve encountered are due to trying to squeeze the ‚Äúextensible mechanism for serializing structured data‚Äù into JSON. Each API response is mostly composed of an array of &amp;quot;Feed Entity&amp;quot; objects like &lt;a href=&quot;https://patrickweaver.net/notes/nyc-subway-feed-entity/&quot;&gt;these&lt;/a&gt;, but there are a few quirks to working with this data (some maybe because of the JSON conversion).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each item in the array has an &lt;code&gt;id&lt;/code&gt; property, but unfortunately these ids do not consistently refer to the same train between each update, it&#39;s best to ignore it.&lt;/li&gt;
&lt;li&gt;The array consists of pairs of objects that either have a &lt;code&gt;tripUpdate&lt;/code&gt; property or a &lt;code&gt;vehicle&lt;/code&gt; property. Each of these have a sub-property called &lt;code&gt;tripId&lt;/code&gt; that allows you to unite the pairs, but there are also some that don&#39;t have a corresponding item (usually these represent trips that recently ended or haven&#39;t yet begun).&lt;/li&gt;
&lt;li&gt;The data mixes together HH:MM:SS timestamps for data about when a train&#39;s trip started, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Unix_time&quot;&gt;Unix timestamps&lt;/a&gt; for data about the current time (according to the API) and when a train will arrive at a station (the API provides both arrival and departure times but as far as I have seen they are always identical).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tripUpdate&lt;/code&gt; items show information about the stops a train will make in the future (stopTimeUpdates) and vehicle items show information about the current status of the train, but the first &lt;code&gt;stopTimeUpdate&lt;/code&gt; is usually in the past.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I had never heard of Protocol Buffers before starting this project, so I was excited to learn more about them while reading through &lt;a href=&quot;https://dataintensive.net/&quot;&gt;Designing Data Intensive Applications&lt;/a&gt; with fellow Recursers.  In the book Martin Kleppmann notes that a, &amp;quot;curious detail of Protocol Buffers is that it does not have a list or array datatype, but instead has a repeated marker for fields (which is a third option alongside required and optional).&amp;quot; This could be the reason for the strange organization of the &lt;code&gt;tripUpdate&lt;/code&gt; and &lt;code&gt;vehicle&lt;/code&gt; properties.&lt;/p&gt;
&lt;h3&gt;Calculating Train Locations&lt;/h3&gt;
&lt;p&gt;The subway real-time API doesn‚Äôt have latitude and longitude data because it is designed to feed data to countdown clock style applications that show when the train will be at a specific station. One of the earliest features that I built into the real-time map was a way to translate these station-by-station countdown clocks into an approximation of  the location of each train. My first attempt at this was to just show a list of stations and display an icon for a train between the names of the station it had been at previously and the station it was approaching.&lt;/p&gt;
&lt;figure&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;img src=&quot;https://patrickweaver.net/images/blog/nyc-subway/prototype-diagram.png&quot; alt=&quot;An early prototype diagram of G train positions.&quot; style=&quot;max-height: 400px; margin: 0 auto;&quot;&gt;
&lt;/div&gt;
&lt;figcaption&gt;A first prototype, still available at: &lt;a href=&quot;https://nyc-subway-g.glitch.me/&quot;&gt;nyc-subway-g.glitch.me&lt;/a&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The next step was plotting the stations on a map. To start off, as with the diagram version, I just placed each train at the midpoint between the station it was travelling from and the station it was travelling towards.&lt;/p&gt;
&lt;p&gt;A goal I had for the project was not just to show real-time train locations, but to animate them as they moved around the map. To determine how long each I should expect each train to take to travel between each station I logged updates from the MTA API for a few hours and noted both the average time for each pair of stations, and the longest time I had seen. I&#39;m still experimenting a little bit with what values to use as the baseline, but from looking at the logged numbers there does seem to be an expected amount of time for most stations.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;strong&gt;G Train Stop Time Distances&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;G: {
   G22: { N: { avg: 60, max: 71 }, S: null },
   G24: { N: { avg: 123, max: 180 }, S: { avg: 60, max: 106 } },
   G26: { N: { avg: 75, max: 90 }, S: { avg: 108, max: 180 } },
   G28: { N: { avg: 128, max: 180 }, S: { avg: 78, max: 90 } },
   G29: { N: { avg: 60, max: 76 }, S: { avg: 139, max: 180 } },
   G30: { N: { avg: 51, max: 74 }, S: { avg: 70, max: 90 } },
   G31: { N: { avg: 60, max: 90 }, S: { avg: 58, max: 69 } },
   G32: { N: { avg: 66, max: 87 }, S: { avg: 53, max: 84 } },
   G33: { N: { avg: 50, max: 66 }, S: { avg: 67, max: 84 } },
   G34: { N: { avg: 58, max: 90 }, S: { avg: 54, max: 66 } },
   G35: { N: { avg: 68, max: 86 }, S: { avg: 50, max: 81 } },
   G36: { N: { avg: 81, max: 161 }, S: { avg: 59, max: 71 } },
   A42: { N: { avg: 70, max: 177 }, S: { avg: 87, max: 157 } },
   F20: { N: { avg: 68, max: 90 }, S: { avg: 86, max: 165 } },
   F21: { N: { avg: 72, max: 120 }, S: { avg: 76, max: 120 } },
   F22: { N: { avg: 62, max: 90 }, S: { avg: 84, max: 120 } },
   F23: { N: { avg: 90, max: 120 }, S: { avg: 88, max: 150 } },
   F24: { N: { avg: 101, max: 120 }, S: { avg: 67, max: 84 } },
   F25: { N: { avg: 139, max: 180 }, S: { avg: 71, max: 90 } },
   F26: { N: { avg: 120, max: 120 }, S: { avg: 109, max: 150 } },
   F27: { N: null, S: { avg: 81, max: 120 } },
 }
&lt;/code&gt;&lt;/pre&gt;
&lt;figcaption&gt;Average and max wait times in seconds for stops on the G line.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Trip Id&lt;/th&gt;
&lt;th&gt;Trip Start Time&lt;/th&gt;
&lt;th&gt;Trip Date&lt;/th&gt;
&lt;th&gt;Route&lt;/th&gt;
&lt;th&gt;Stop1 Arrival&lt;/th&gt;
&lt;th&gt;Stop1 Id&lt;/th&gt;
&lt;th&gt;Stop2 Arrival&lt;/th&gt;
&lt;th&gt;Stop2 Id&lt;/th&gt;
&lt;th&gt;Seconds&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;073476_G..N&lt;/td&gt;
&lt;td&gt;12:14:46&lt;/td&gt;
&lt;td&gt;20200818&lt;/td&gt;
&lt;td&gt;G&lt;/td&gt;
&lt;td&gt;1597768631&lt;/td&gt;
&lt;td&gt;G33N&lt;/td&gt;
&lt;td&gt;1597768692&lt;/td&gt;
&lt;td&gt;G32N&lt;/td&gt;
&lt;td&gt;61&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;074600_G..N&lt;/td&gt;
&lt;td&gt;12:26:00&lt;/td&gt;
&lt;td&gt;20200818&lt;/td&gt;
&lt;td&gt;G&lt;/td&gt;
&lt;td&gt;1597769103&lt;/td&gt;
&lt;td&gt;G33N&lt;/td&gt;
&lt;td&gt;1597769172&lt;/td&gt;
&lt;td&gt;G32N&lt;/td&gt;
&lt;td&gt;69&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;075000_G..N&lt;/td&gt;
&lt;td&gt;12:30:00&lt;/td&gt;
&lt;td&gt;20200818&lt;/td&gt;
&lt;td&gt;G&lt;/td&gt;
&lt;td&gt;1597769531&lt;/td&gt;
&lt;td&gt;G33N&lt;/td&gt;
&lt;td&gt;1597769596&lt;/td&gt;
&lt;td&gt;G32N&lt;/td&gt;
&lt;td&gt;65&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;076501_G..N&lt;/td&gt;
&lt;td&gt;12:45:01&lt;/td&gt;
&lt;td&gt;20200818&lt;/td&gt;
&lt;td&gt;G&lt;/td&gt;
&lt;td&gt;1597770333&lt;/td&gt;
&lt;td&gt;G33N&lt;/td&gt;
&lt;td&gt;1597770396&lt;/td&gt;
&lt;td&gt;G32N&lt;/td&gt;
&lt;td&gt;63&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;077700_G..N&lt;/td&gt;
&lt;td&gt;12:57:00&lt;/td&gt;
&lt;td&gt;20200818&lt;/td&gt;
&lt;td&gt;G&lt;/td&gt;
&lt;td&gt;1597771043&lt;/td&gt;
&lt;td&gt;G33N&lt;/td&gt;
&lt;td&gt;1597771104&lt;/td&gt;
&lt;td&gt;G32N&lt;/td&gt;
&lt;td&gt;61&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;078403_G..N&lt;/td&gt;
&lt;td&gt;13:04:02&lt;/td&gt;
&lt;td&gt;20200818&lt;/td&gt;
&lt;td&gt;G&lt;/td&gt;
&lt;td&gt;1597771443&lt;/td&gt;
&lt;td&gt;G33N&lt;/td&gt;
&lt;td&gt;1597771524&lt;/td&gt;
&lt;td&gt;G32N&lt;/td&gt;
&lt;td&gt;81&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;079600_G..N&lt;/td&gt;
&lt;td&gt;13:16:00&lt;/td&gt;
&lt;td&gt;20200818&lt;/td&gt;
&lt;td&gt;G&lt;/td&gt;
&lt;td&gt;1597772051&lt;/td&gt;
&lt;td&gt;G33N&lt;/td&gt;
&lt;td&gt;1597772112&lt;/td&gt;
&lt;td&gt;G32N&lt;/td&gt;
&lt;td&gt;61&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;080550_G..N&lt;/td&gt;
&lt;td&gt;13:25:30&lt;/td&gt;
&lt;td&gt;20200818&lt;/td&gt;
&lt;td&gt;G&lt;/td&gt;
&lt;td&gt;1597772711&lt;/td&gt;
&lt;td&gt;G33N&lt;/td&gt;
&lt;td&gt;1597772776&lt;/td&gt;
&lt;td&gt;G32N&lt;/td&gt;
&lt;td&gt;65&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;figcaption&gt;Logged travel Times between the Bedford - Nostrand stop and the Myrtle - Willoughby stop on the G train&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3&gt;Secret Stations&lt;/h3&gt;
&lt;p&gt;One thing I discovered while logging updates from the MTA API was that it contained secret stations! The MTA provides a &lt;a href=&quot;http://web.mta.info/developers/data/nyct/subway/Stations.csv&quot;&gt;list of all of the stations in the system&lt;/a&gt; with data like latitude and longitude. Each station has an ID (see Stop1 and Stop2 id in the diagram above and called &amp;quot;GTFS Stop ID&amp;quot; in the list). The stop IDs are a letter and 2 numbers, with the letter often corresponding to the line it serves (or used to historically), and the numbers mostly occurring in sequence. but some trains would have planned &amp;quot;stops&amp;quot; at stations that weren&#39;t in the list! My best guess is that these stations are something station-like in the MTA&#39;s infrastructure, which usually appear near the end of a line.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;quot;H17&amp;quot; is between Howard Beach/JFK Airport and Broad Channel on the A and Rockaway Shuttle and is likely where the Shuttle trains go to turn around.&lt;/li&gt;
&lt;li&gt;&amp;quot;H19&amp;quot; is before the Broad Channel stop on the A and Rockaway Shuttle and may also be related to Shuttle turnaround?&lt;/li&gt;
&lt;li&gt;&amp;quot;H18&amp;quot; and &amp;quot;H05&amp;quot; are between Broad Channel and Beach 67 St on the A and Rockaway Shuttle, which may have to do with tracks that run between Beach 67 and Beach 90 Sts.&lt;/li&gt;
&lt;li&gt;&amp;quot;A29&amp;quot; is between Penn Station and 14th St on the A, C, and E, which is strange because the C and E stop at 23rd St., which is also between those stations, but has the ID &amp;quot;A30&amp;quot;.&lt;/li&gt;
&lt;li&gt;&amp;quot;A39&amp;quot; is between Fulton St. and High St. on the A and C, which might have something to do with the track stubs on the Brooklyn side (one of which is the NYC Transit museum).&lt;/li&gt;
&lt;li&gt;&amp;quot;A58&amp;quot; is between Grant Av. and 80th St. on the A, which, is where the A train emerges from a tunnel to run on elevated tracks.&lt;/li&gt;
&lt;li&gt;&amp;quot;A62&amp;quot; is between Rockaway Blvd. and 104th St. on the A and probably has something to do with the merging between the 3 versions of the A at Rockaway Blvd.&lt;/li&gt;
&lt;li&gt;&amp;quot;R60&amp;quot; is between Queensboro Plaza and Lexington Ave/59th St. on the N, R, and W. My guess is that this has something to do with the N/W and R tracks merging before going into a tunnel.&lt;/li&gt;
&lt;li&gt;&amp;quot;R65&amp;quot; is between the Whitehall St. and Court St. stops on the R, and could also be related to the same track stubs as &amp;quot;A39&amp;quot;.&lt;/li&gt;
&lt;li&gt;&amp;quot;B24&amp;quot; is between Bay 50th St and Coney Island on the D, and is probably the MTA Coney Island Yard.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Drawing the Static Map&lt;/h3&gt;
&lt;p&gt;I wasn&#39;t quite satisfied with the angular paths that drawing lines directly between stations created, and I was fortunately able to find &lt;a href=&quot;https://github.com/blahblahblah-/theweekendest&quot;&gt;Sunny Ng&#39;s advice&lt;/a&gt; on extracting shape arrays from the non real-time GTFS MTA data. Using these shape arrays I could draw route maps with smooth curves, and even animate trains along those curves. But one of the things that makes a subway map a subway map is seeing lines that run on the same tracks as parallel lines. I also wanted to double these lines and show Northbound and Southbound trains on separate tracks (something that the new MTA map fails to do).&lt;/p&gt;
&lt;p&gt;After trying to approach the parallel lines problem geometrically I was pointed in the right direction by a fellow RC participant and was able to draw great looking lines by treating the Latitude/Longitude points in the shape arrays as vectors (More on this in my &lt;a href=&quot;https://doodles.patrickweaver.net/drawing-parallel-lines-on-a-map/&quot;&gt;interactive slides on this problem&lt;/a&gt; and more on the &lt;a href=&quot;https://medium.com/transit-app/how-we-built-the-worlds-prettiest-auto-generated-transit-maps-12d0c6fa502f&quot;&gt;challenge of drawing nice train lines from the Transit app&lt;/a&gt;).&lt;/p&gt;
&lt;figure&gt;
&lt;div style=&quot;display: flex; max-width: 100%;&quot;&gt;
  &lt;img src=&quot;https://patrickweaver.net/images/blog/nyc-subway/nyc-subway-f-g.png&quot; alt=&quot;A screenshot of my map.&quot; style=&quot;max-height: 500px; max-width: 49%; margin: 0 auto;&quot;&gt;
  &lt;span style=&quot;width: 5px;&quot;&gt;&lt;/span&gt;
  &lt;img src=&quot;https://patrickweaver.net/images/blog/nyc-subway/mta-f-g.png&quot; alt=&quot;A screenshot of the MTA map.&quot; style=&quot;max-height: 500px; max-width: 49%; margin: 0 auto;&quot;&gt;
&lt;/div&gt;
&lt;figcaption&gt;Similar sections of my map and the MTA map&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3&gt;A Prototype&lt;/h3&gt;
&lt;p&gt;My map is online at &lt;a href=&quot;https://nyc-subway.glitch.me/&quot;&gt;nyc-subway.glitch.me&lt;/a&gt;, bugs and all. I did most of the work on the map using only the G train API endpoint. This was a helpful limitation when i was first experimenting with what was possible using the data, but may have led to more bugs because of the slight differences in the data available for each set of lines.&lt;/p&gt;
&lt;p&gt;A common complaint about the new official real-time map is that it seems to use as much computer power as it can. My map isn&#39;t much better because it is doing all of the geographic calculations in the user&#39;s browser, my guess is that the MTA&#39;s map is also. One update I might take on over the next week and a half as my time at RC winds down is moving these calculations to a server, and sending only train position changes to the map visualization. This may also help with the bug my current prototype exhibits where leaving and coming back to the tab a few minutes later will cause trains to fly around the map without regard for the lines or stations.&lt;/p&gt;
&lt;p&gt;The MTA data is weird because it&#39;s created by a system that could never have anticipated the kind of systems that now try to contain it. Overall, working with and working around the weirdness in the data has been challenging, but a great reminder that the most interesting real-world problems are often hard to jam into our brittle computer systems, and that&#39;s probably a good thing.&lt;/p&gt;
</content>
  </entry>
</feed>